{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8cf01df-787c-49ef-83f2-e08bce88506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c6b4896-2494-4d85-ae7b-96ab875f251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd -q .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3c2921e-45c2-4080-8c90-089daeeceba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lcdb\n",
    "import json\n",
    "import lcpfn \n",
    "import torch as th\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e94a1c12-bd40-4c4e-8f3c-ccb2f91ab02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curve = lcdb.get_curve(3, \"sklearn.linear_model.LogisticRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac6fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDatasetJson(path):\n",
    "    f = open(path)\n",
    "\n",
    "    dataset = json.read(f)\n",
    "    \n",
    "    for k in dataset.keys():\n",
    "        #TURN THEM INTO INT\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c6759bb-1fbe-4c5f-a6d2-4e736c555596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 23, 32, 45, 64, 91, 128, 181, 256, 362, 512, 724, 1024, 1448, 2048, 2588]\n",
      "[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.913, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9565, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9565, 1.0, 1.0, 1.0, 1.0, 0.9565, 1.0, 1.0, 1.0, 1.0, 0.9565, 0.9565, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9565, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9565, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9565, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9565, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9565, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9565, 1.0, 1.0, 1.0, 1.0, 0.9565, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 0.9688, 1.0, 1.0, 1.0, 0.9688, 1.0, 0.9688, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9688, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9688, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9688, 1.0, 1.0, 0.9688, 1.0, 1.0, 0.9688, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9688, 1.0, 1.0, 0.9688, 0.9688, 0.9688, 1.0, 0.9688, 1.0, 1.0, 1.0, 1.0, 0.9688, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9688, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9688, 1.0, 0.9688, 0.9688, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9688, 1.0, 0.9375, 1.0, 1.0, 0.9688, 0.9688, 1.0, 0.9688, 1.0, 1.0, 0.9688, 0.9688, 1.0, 1.0, 0.9688, 1.0, 1.0, 1.0, 1.0, 0.9688, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 0.9778, 1.0, 1.0, 0.9778, 0.9778, 0.9778, 0.9778, 0.9778, 0.9778, 1.0, 1.0, 0.9556, 0.9556, 1.0, 0.9556, 1.0, 1.0, 1.0, 0.9778, 1.0, 1.0, 1.0, 0.9778, 0.9556, 1.0, 1.0, 1.0, 0.9556, 1.0, 1.0, 0.9778, 0.9556, 1.0, 0.9778, 0.9778, 1.0, 1.0, 0.9778, 0.9333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9556, 0.9778, 0.9778, 1.0, 0.9333, 1.0, 1.0, 1.0, 1.0, 0.9778, 0.9778, 1.0, 1.0, 0.9778, 0.9778, 0.9778, 0.9778, 1.0, 0.9778, 0.9778, 0.9778, 1.0, 0.9778, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9556, 1.0, 0.9778, 1.0, 0.9778, 0.9778, 0.9778, 1.0, 0.9778, 1.0, 1.0, 0.9778, 1.0, 1.0, 1.0, 0.9778, 0.9778, 1.0, 0.9556, 1.0, 1.0, 0.9556, 0.9556, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9778, 1.0, 1.0, 0.9778, 1.0, 1.0, 1.0, 1.0, 0.9778, 0.9778, 1.0, 0.9778, 0.9556, 1.0, 1.0, 1.0, 0.9556, 1.0], [0.9844, 1.0, 0.9844, 0.9531, 1.0, 1.0, 0.9844, 1.0, 1.0, 0.9688, 0.9844, 1.0, 0.9688, 0.9844, 0.9219, 1.0, 0.9844, 1.0, 0.9844, 1.0, 0.9688, 1.0, 0.9844, 0.9844, 0.9844, 0.9688, 1.0, 0.9844, 1.0, 0.9844, 1.0, 0.9375, 0.9844, 0.9688, 1.0, 0.9844, 0.9688, 1.0, 1.0, 0.9844, 0.9375, 1.0, 1.0, 0.9688, 0.9688, 0.9844, 0.9844, 0.9688, 0.9688, 0.9844, 1.0, 0.9375, 1.0, 0.9844, 0.9844, 0.9844, 0.9844, 0.9688, 1.0, 1.0, 0.9688, 0.9688, 0.9844, 0.9844, 0.9844, 0.9844, 0.9688, 0.9688, 1.0, 0.9844, 0.9844, 0.9688, 0.9688, 0.9844, 1.0, 1.0, 0.9844, 1.0, 0.9844, 0.9531, 1.0, 0.9844, 1.0, 0.9844, 1.0, 0.9375, 1.0, 0.9844, 0.9688, 0.9531, 0.9844, 0.9844, 0.9844, 1.0, 0.9844, 0.9844, 0.9844, 0.9531, 1.0, 1.0, 1.0, 0.9688, 1.0, 0.9375, 1.0, 0.9844, 0.9688, 0.9688, 1.0, 0.9688, 0.9688, 1.0, 0.9688, 0.9844, 1.0, 1.0, 0.9844, 1.0, 0.9531, 0.9688, 0.9844, 0.9844, 1.0, 0.9688, 1.0], [1.0, 0.989, 0.978, 0.967, 1.0, 1.0, 0.978, 0.989, 1.0, 0.978, 0.967, 1.0, 0.978, 0.978, 0.9451, 0.989, 0.967, 1.0, 0.978, 1.0, 0.9451, 0.989, 0.978, 0.989, 0.989, 0.967, 0.989, 1.0, 1.0, 0.978, 0.989, 0.9451, 0.978, 0.978, 1.0, 0.978, 0.978, 0.989, 0.956, 0.978, 0.956, 0.967, 0.956, 0.978, 0.978, 0.989, 0.967, 0.989, 0.978, 0.956, 0.978, 0.9451, 0.9341, 0.978, 0.978, 0.989, 0.978, 0.989, 0.989, 0.956, 0.978, 0.967, 0.978, 0.989, 0.989, 0.967, 0.978, 0.989, 0.989, 0.989, 0.967, 0.978, 0.978, 0.989, 1.0, 1.0, 0.989, 0.967, 0.978, 0.967, 0.989, 0.989, 1.0, 0.956, 0.989, 0.967, 0.978, 0.989, 0.967, 0.967, 0.967, 0.978, 0.978, 0.978, 0.9451, 0.989, 0.978, 0.967, 1.0, 0.989, 0.989, 0.967, 0.989, 0.956, 0.956, 0.989, 0.989, 0.978, 0.989, 0.978, 0.967, 0.978, 0.978, 0.978, 0.989, 0.978, 0.967, 0.989, 0.967, 0.9341, 0.978, 0.967, 0.967, 0.989, 0.978], [0.9688, 0.9922, 0.9766, 0.9688, 0.9922, 0.9844, 0.9766, 0.9844, 0.9766, 0.9844, 0.9688, 1.0, 0.9688, 0.9688, 0.9375, 0.9844, 0.9688, 0.9844, 0.9844, 0.9844, 0.9609, 0.9766, 1.0, 1.0, 0.9688, 0.9688, 0.9922, 0.9844, 0.9844, 0.9688, 0.9766, 0.9531, 0.9453, 0.9844, 0.9844, 0.9688, 0.9688, 0.9766, 0.9766, 0.9922, 0.9531, 0.9531, 0.9531, 0.9688, 0.9844, 1.0, 0.9688, 0.9844, 0.9766, 0.9688, 0.9766, 0.9453, 0.9531, 0.9688, 0.9844, 0.9844, 0.9844, 0.9766, 0.9844, 0.9688, 0.9766, 0.9453, 0.9453, 0.9688, 1.0, 0.9844, 0.9766, 1.0, 0.9766, 0.9844, 0.9766, 0.9531, 0.9844, 0.9922, 1.0, 1.0, 0.9844, 0.9609, 0.9844, 0.9844, 0.9844, 0.9844, 1.0, 0.9531, 0.9844, 0.9766, 0.9766, 0.9688, 0.9766, 0.9688, 0.9844, 0.9688, 0.9844, 0.9844, 0.9688, 0.9922, 0.9766, 0.9766, 1.0, 0.9844, 0.9688, 0.9766, 0.9922, 0.9688, 0.9688, 0.9844, 0.9922, 0.9609, 0.9922, 0.9453, 0.9844, 0.9844, 0.9766, 0.9766, 0.9844, 0.9844, 0.9609, 0.9844, 0.9609, 0.9609, 0.9922, 0.9766, 0.9609, 1.0, 0.9688], [0.9779, 0.9724, 0.9834, 0.9724, 0.9724, 0.9834, 0.9834, 0.9613, 0.9669, 0.9669, 0.9834, 0.989, 0.9613, 0.9779, 0.9669, 0.9834, 0.9779, 0.9669, 0.9834, 0.989, 0.9558, 0.9724, 0.9724, 0.9834, 0.9669, 0.9613, 0.989, 0.9834, 0.989, 0.9669, 0.9724, 0.9392, 0.9613, 0.9724, 0.9945, 0.9724, 0.9724, 0.9669, 0.9669, 0.9669, 0.9558, 0.9669, 0.9613, 0.9779, 0.9613, 0.9724, 0.9724, 0.9724, 0.9613, 0.9669, 0.9724, 0.9448, 0.9503, 0.9834, 0.9669, 0.9779, 0.9669, 0.9669, 0.9669, 0.9558, 0.9503, 0.9669, 0.9669, 0.9724, 0.9724, 0.989, 0.9669, 0.9945, 0.9724, 0.9613, 0.9669, 0.9779, 0.9834, 0.9945, 0.9834, 0.989, 0.9724, 0.9779, 0.989, 0.9669, 0.9834, 0.989, 0.9669, 0.9558, 0.9834, 0.9779, 0.9613, 0.9669, 0.9724, 0.9558, 0.9779, 0.9724, 0.9669, 0.9558, 0.9503, 0.9834, 0.9613, 0.9724, 0.9779, 0.9669, 0.9669, 0.9613, 0.9834, 0.9669, 0.9613, 0.9724, 0.9834, 0.9503, 0.9503, 0.9669, 0.9834, 0.989, 0.9779, 0.9779, 0.9779, 0.9669, 0.9613, 0.9834, 0.9779, 0.9613, 0.9779, 0.9724, 0.9724, 0.9945, 0.9779], [0.9727, 0.9727, 0.9688, 0.9688, 0.9727, 0.9727, 0.9805, 0.9688, 0.9609, 0.9648, 0.9766, 0.9844, 0.9492, 0.9688, 0.9688, 0.9844, 0.9805, 0.9688, 0.9727, 0.9844, 0.9688, 0.9688, 0.9688, 0.9844, 0.9492, 0.9648, 0.9844, 0.9609, 0.9766, 0.9648, 0.9766, 0.9453, 0.957, 0.9766, 0.9805, 0.9688, 0.9648, 0.957, 0.9805, 0.9688, 0.9648, 0.9688, 0.957, 0.9766, 0.957, 0.9766, 0.9609, 0.9727, 0.9648, 0.9609, 0.957, 0.9414, 0.957, 0.9688, 0.9492, 0.9648, 0.9609, 0.957, 0.957, 0.9609, 0.9453, 0.9609, 0.9727, 0.9727, 0.9648, 0.9648, 0.9531, 0.9805, 0.9688, 0.9648, 0.957, 0.9648, 0.9688, 0.9922, 0.9727, 0.9805, 0.9609, 0.9727, 0.9727, 0.9609, 0.9727, 0.9844, 0.9766, 0.9609, 0.9648, 0.9727, 0.9688, 0.9688, 0.9766, 0.957, 0.9805, 0.9844, 0.9727, 0.9688, 0.9648, 0.9727, 0.9648, 0.9648, 0.9648, 0.9688, 0.957, 0.9688, 0.9766, 0.9688, 0.9609, 0.9805, 0.9766, 0.9609, 0.9688, 0.9688, 0.9766, 0.9766, 0.9609, 0.9648, 0.9727, 0.9531, 0.957, 0.9766, 0.9727, 0.9609, 0.9805, 0.9688, 0.9688, 0.9805, 0.9609], [0.9779, 0.9669, 0.9641, 0.9558, 0.9641, 0.9807, 0.9641, 0.9641, 0.9641, 0.9613, 0.9641, 0.9586, 0.9558, 0.9669, 0.9724, 0.9586, 0.9669, 0.9613, 0.9613, 0.9696, 0.9724, 0.9669, 0.9751, 0.9862, 0.9613, 0.9558, 0.9696, 0.9641, 0.9613, 0.9669, 0.9696, 0.9503, 0.9613, 0.9641, 0.9779, 0.9669, 0.9613, 0.9641, 0.9779, 0.9613, 0.9696, 0.9669, 0.9696, 0.9724, 0.9641, 0.9724, 0.953, 0.9724, 0.9724, 0.9558, 0.9558, 0.9448, 0.9696, 0.9641, 0.9558, 0.9696, 0.9475, 0.9558, 0.9669, 0.9696, 0.9503, 0.9669, 0.9751, 0.9586, 0.9724, 0.9724, 0.9613, 0.9696, 0.9724, 0.9558, 0.9586, 0.9641, 0.9558, 0.989, 0.9696, 0.9751, 0.9586, 0.9724, 0.9751, 0.9613, 0.9779, 0.9779, 0.9807, 0.9613, 0.9696, 0.9669, 0.9669, 0.9807, 0.9641, 0.953, 0.9724, 0.9807, 0.9724, 0.9724, 0.9613, 0.9751, 0.9696, 0.9641, 0.9696, 0.9613, 0.9641, 0.9613, 0.9807, 0.9641, 0.9669, 0.9641, 0.9696, 0.9503, 0.9807, 0.9724, 0.953, 0.9724, 0.9669, 0.9641, 0.9613, 0.9586, 0.9641, 0.9751, 0.9669, 0.9586, 0.9779, 0.9613, 0.9696, 0.9862, 0.9586], [0.9648, 0.9688, 0.9688, 0.9629, 0.9648, 0.9707, 0.957, 0.9648, 0.9609, 0.9688, 0.9668, 0.959, 0.9648, 0.959, 0.9766, 0.9707, 0.9688, 0.9648, 0.9668, 0.9648, 0.9629, 0.9688, 0.9668, 0.9746, 0.9668, 0.957, 0.9668, 0.9629, 0.959, 0.9707, 0.9707, 0.959, 0.9688, 0.9648, 0.9727, 0.9668, 0.9668, 0.9688, 0.9668, 0.9648, 0.9668, 0.9629, 0.9629, 0.9746, 0.9707, 0.9746, 0.9629, 0.9688, 0.9707, 0.959, 0.959, 0.9492, 0.9629, 0.9688, 0.9668, 0.9785, 0.9551, 0.9629, 0.9551, 0.9609, 0.9492, 0.9609, 0.9766, 0.9668, 0.9629, 0.9707, 0.9629, 0.9707, 0.9785, 0.9648, 0.9609, 0.9707, 0.957, 0.9785, 0.9609, 0.9688, 0.9668, 0.9688, 0.9766, 0.9609, 0.9668, 0.9805, 0.9785, 0.9727, 0.9648, 0.9668, 0.9746, 0.9746, 0.9648, 0.9551, 0.9727, 0.9805, 0.9648, 0.9727, 0.9414, 0.9785, 0.9707, 0.9609, 0.9707, 0.9766, 0.9512, 0.957, 0.9727, 0.9727, 0.9688, 0.9629, 0.9766, 0.957, 0.9766, 0.9785, 0.959, 0.9688, 0.9609, 0.9707, 0.959, 0.9707, 0.9688, 0.9609, 0.9551, 0.9551, 0.9805, 0.9648, 0.9688, 0.9805, 0.9707], [0.9627, 0.9655, 0.9738, 0.9641, 0.9751, 0.9696, 0.9613, 0.9669, 0.9613, 0.9558, 0.9751, 0.9586, 0.9599, 0.9558, 0.9738, 0.9765, 0.9696, 0.9641, 0.9613, 0.9669, 0.9586, 0.9669, 0.9627, 0.9696, 0.9696, 0.9682, 0.9696, 0.9627, 0.9696, 0.971, 0.9613, 0.9613, 0.9655, 0.9599, 0.9655, 0.9655, 0.9669, 0.9696, 0.9696, 0.9641, 0.9599, 0.9696, 0.9682, 0.9669, 0.9641, 0.9682, 0.9641, 0.9696, 0.9641, 0.9641, 0.9641, 0.9627, 0.9586, 0.9599, 0.9641, 0.9655, 0.9641, 0.9655, 0.9572, 0.9682, 0.9599, 0.9655, 0.9779, 0.9751, 0.9641, 0.9793, 0.9586, 0.9751, 0.9751, 0.9627, 0.9641, 0.9627, 0.9599, 0.9738, 0.9669, 0.971, 0.9627, 0.9724, 0.971, 0.9724, 0.9696, 0.971, 0.9738, 0.9682, 0.9696, 0.9669, 0.9696, 0.9738, 0.9682, 0.9558, 0.9724, 0.9738, 0.9558, 0.9696, 0.953, 0.9669, 0.9724, 0.9655, 0.9682, 0.9696, 0.9572, 0.9669, 0.9696, 0.9696, 0.9738, 0.9586, 0.9765, 0.9627, 0.9765, 0.9751, 0.9613, 0.9765, 0.9613, 0.9669, 0.9599, 0.9724, 0.9599, 0.9641, 0.9641, 0.9696, 0.971, 0.9682, 0.9696, 0.9682, 0.9613], [0.9688, 0.9609, 0.9697, 0.9619, 0.9736, 0.9697, 0.9668, 0.9668, 0.9717, 0.96, 0.9756, 0.9619, 0.96, 0.9629, 0.9736, 0.9717, 0.9678, 0.9658, 0.9678, 0.9648, 0.9648, 0.9678, 0.9639, 0.9727, 0.9629, 0.9697, 0.9668, 0.9668, 0.9668, 0.9648, 0.9688, 0.9639, 0.9609, 0.96, 0.9736, 0.9629, 0.9668, 0.9697, 0.9639, 0.9727, 0.9678, 0.9648, 0.9717, 0.9668, 0.9668, 0.9639, 0.9629, 0.9717, 0.9658, 0.9678, 0.9688, 0.9619, 0.9609, 0.9658, 0.9639, 0.9668, 0.9639, 0.9648, 0.9609, 0.96, 0.9561, 0.9619, 0.9736, 0.9678, 0.96, 0.9717, 0.9619, 0.9678, 0.9736, 0.96, 0.9678, 0.9668, 0.9658, 0.9717, 0.9668, 0.9697, 0.957, 0.9678, 0.9678, 0.9668, 0.9639, 0.9717, 0.9629, 0.9668, 0.9648, 0.9639, 0.9639, 0.9648, 0.9736, 0.9619, 0.9707, 0.9697, 0.958, 0.9717, 0.957, 0.957, 0.9697, 0.9688, 0.9727, 0.9678, 0.9658, 0.9697, 0.9658, 0.9707, 0.9639, 0.959, 0.9736, 0.9639, 0.9658, 0.9756, 0.9639, 0.9668, 0.96, 0.9561, 0.9561, 0.9668, 0.9639, 0.9629, 0.9717, 0.9658, 0.9697, 0.9619, 0.9717, 0.9648, 0.9697], [0.9662, 0.9634, 0.9717, 0.9634, 0.9675, 0.9662, 0.9675, 0.9689, 0.9675, 0.9669, 0.9696, 0.9634, 0.9655, 0.9634, 0.9703, 0.9738, 0.9662, 0.9648, 0.9682, 0.9634, 0.9689, 0.9662, 0.9627, 0.9696, 0.9627, 0.9634, 0.962, 0.9599, 0.9662, 0.9662, 0.9689, 0.9634, 0.9606, 0.9641, 0.9744, 0.962, 0.9682, 0.9662, 0.9655, 0.9655, 0.9641, 0.9662, 0.9703, 0.9717, 0.9648, 0.971, 0.9669, 0.9731, 0.9682, 0.9703, 0.9682, 0.9662, 0.9627, 0.9648, 0.9675, 0.9675, 0.9641, 0.971, 0.9613, 0.9613, 0.9599, 0.9613, 0.9669, 0.9675, 0.9627, 0.9689, 0.9641, 0.9669, 0.971, 0.9648, 0.9655, 0.9655, 0.9696, 0.9689, 0.9606, 0.9689, 0.9662, 0.9627, 0.9696, 0.9662, 0.9682, 0.9703, 0.9648, 0.9703, 0.971, 0.9655, 0.9675, 0.9627, 0.9696, 0.9703, 0.971, 0.9703, 0.9606, 0.9675, 0.962, 0.9662, 0.9724, 0.9689, 0.9703, 0.9689, 0.9682, 0.9648, 0.9669, 0.9689, 0.9641, 0.9606, 0.9675, 0.9669, 0.9669, 0.9669, 0.9669, 0.9696, 0.9675, 0.9634, 0.9655, 0.9627, 0.9648, 0.9634, 0.9724, 0.9655, 0.9613, 0.9675, 0.9675, 0.9717, 0.971], [0.9663, 0.9702, 0.9731, 0.9683, 0.9702, 0.9683, 0.9697, 0.9688, 0.9678, 0.9688, 0.9683, 0.9653, 0.9688, 0.9673, 0.9688, 0.9702, 0.9702, 0.9731, 0.9673, 0.9673, 0.9648, 0.9658, 0.9644, 0.9688, 0.9663, 0.9648, 0.9683, 0.9678, 0.9634, 0.9668, 0.9678, 0.9658, 0.9634, 0.9683, 0.9717, 0.9658, 0.9673, 0.9736, 0.9727, 0.9673, 0.9683, 0.9673, 0.9707, 0.9678, 0.9644, 0.9688, 0.9683, 0.9692, 0.9648, 0.9702, 0.9663, 0.9658, 0.9644, 0.9668, 0.9624, 0.9683, 0.9688, 0.9702, 0.9648, 0.9658, 0.9663, 0.9634, 0.9663, 0.9688, 0.9653, 0.9668, 0.9653, 0.9648, 0.9707, 0.9634, 0.9663, 0.9634, 0.9648, 0.9658, 0.9668, 0.9668, 0.9717, 0.9653, 0.9707, 0.9697, 0.9668, 0.9692, 0.9668, 0.9707, 0.9692, 0.9688, 0.9712, 0.9673, 0.9692, 0.9727, 0.9673, 0.9736, 0.9668, 0.9683, 0.9644, 0.9697, 0.9717, 0.9702, 0.9673, 0.9653, 0.9683, 0.9663, 0.9678, 0.9663, 0.9663, 0.9619, 0.9688, 0.9678, 0.9663, 0.9658, 0.9697, 0.9692, 0.9683, 0.9648, 0.9658, 0.9639, 0.9663, 0.9658, 0.9678, 0.9678, 0.9639, 0.9658, 0.9692, 0.9707, 0.9736], [0.9695, 0.9706, 0.9722, 0.9687, 0.9706, 0.9699, 0.9691, 0.9702, 0.9699, 0.9706, 0.971, 0.9706, 0.9706, 0.9683, 0.9679, 0.9695, 0.9695, 0.9702, 0.9691, 0.9672, 0.9687, 0.9695, 0.9668, 0.9706, 0.9675, 0.9702, 0.9672, 0.9664, 0.9672, 0.9691, 0.9668, 0.9672, 0.9679, 0.9664, 0.9695, 0.9668, 0.9691, 0.9687, 0.9706, 0.9691, 0.9672, 0.9672, 0.9714, 0.9668, 0.9656, 0.9687, 0.9699, 0.9664, 0.9652, 0.9679, 0.9664, 0.9656, 0.9648, 0.9675, 0.9664, 0.9672, 0.9702, 0.971, 0.9664, 0.9679, 0.9687, 0.9648, 0.966, 0.9683, 0.9668, 0.9679, 0.9679, 0.966, 0.9664, 0.9679, 0.9664, 0.9691, 0.9668, 0.9664, 0.9699, 0.9679, 0.9706, 0.9664, 0.9691, 0.9706, 0.9691, 0.9672, 0.9672, 0.9687, 0.9672, 0.971, 0.971, 0.9687, 0.966, 0.9699, 0.9664, 0.9691, 0.9699, 0.9714, 0.966, 0.9706, 0.9737, 0.9691, 0.9695, 0.9706, 0.9695, 0.9679, 0.9683, 0.9664, 0.9679, 0.9679, 0.9687, 0.9679, 0.9683, 0.9668, 0.9683, 0.9683, 0.9695, 0.9706, 0.9679, 0.9679, 0.9683, 0.9672, 0.9679, 0.9679, 0.9695, 0.9664, 0.9675, 0.971, 0.9695]]\n",
      "[[0.6458, 0.7708, 0.5694, 0.6111, 0.6597, 0.5417, 0.6111, 0.6562, 0.5312, 0.6979, 0.6181, 0.7222, 0.7118, 0.5903, 0.6007, 0.5694, 0.6146, 0.6493, 0.5278, 0.5486, 0.6979, 0.6944, 0.6701, 0.6146, 0.7639, 0.7222, 0.7569, 0.7083, 0.5903, 0.6424, 0.5660000000000001, 0.6667, 0.6458, 0.5903, 0.7188, 0.7014, 0.7396, 0.7326, 0.7292, 0.6597, 0.6319, 0.691, 0.5903, 0.5625, 0.7674, 0.6458, 0.7222, 0.7118, 0.6111, 0.7465, 0.7083, 0.7153, 0.6771, 0.6215, 0.7361, 0.5451, 0.6632, 0.7778, 0.5556, 0.7257, 0.7153, 0.6146, 0.6389, 0.6458, 0.7396, 0.6181, 0.6979, 0.7118, 0.5556, 0.6806, 0.6389, 0.6562, 0.6806, 0.5694, 0.691, 0.7222, 0.7014, 0.5521, 0.6632, 0.6667, 0.5451, 0.6806, 0.6458, 0.5660000000000001, 0.7604, 0.7535, 0.6354, 0.7361, 0.6285, 0.7257, 0.5972, 0.6354, 0.6944, 0.5278, 0.7396, 0.6667, 0.6632, 0.6806, 0.6319, 0.6667, 0.7153, 0.6597, 0.684, 0.5278, 0.691, 0.5972, 0.6215, 0.7778, 0.5625, 0.6562, 0.6562, 0.6875, 0.7118, 0.6076, 0.7361, 0.625, 0.6354, 0.7326, 0.5694, 0.6806, 0.7049, 0.6215, 0.6944, 0.6215, 0.7535], [0.7847, 0.7917, 0.6424, 0.6979, 0.7743, 0.6562, 0.7535, 0.6875, 0.5972, 0.7569, 0.7222, 0.7431, 0.7674, 0.6285, 0.7326, 0.5938, 0.75, 0.6701, 0.6146, 0.6111, 0.7118, 0.7049, 0.684, 0.6806, 0.7847, 0.7604, 0.8021, 0.7569, 0.7083, 0.7326, 0.6493, 0.7292, 0.7396, 0.6042, 0.7326, 0.7326, 0.7743, 0.7326, 0.7708, 0.7674, 0.7361, 0.7326, 0.6493, 0.6424, 0.7049, 0.6597, 0.7118, 0.7014, 0.6493, 0.7708, 0.7326, 0.7292, 0.7361, 0.7674, 0.7396, 0.6285, 0.6979, 0.816, 0.625, 0.7465, 0.7604, 0.6597, 0.7083, 0.691, 0.7778, 0.7535, 0.6493, 0.7257, 0.7431, 0.684, 0.6667, 0.6771, 0.7361, 0.6389, 0.6389, 0.7674, 0.7049, 0.6458, 0.7222, 0.7396, 0.7153, 0.6632, 0.6979, 0.5972, 0.7569, 0.7708, 0.7222, 0.7917, 0.691, 0.6979, 0.6875, 0.7639, 0.7882, 0.6944, 0.7674, 0.7431, 0.7292, 0.7049, 0.691, 0.7083, 0.7153, 0.7014, 0.7743, 0.7014, 0.7882, 0.6701, 0.6458, 0.8299, 0.6389, 0.7778, 0.6736, 0.8125, 0.6944, 0.6944, 0.7569, 0.7049, 0.6875, 0.7361, 0.6632, 0.7188, 0.8056, 0.7014, 0.7882, 0.6181, 0.7882], [0.8264, 0.8229, 0.7153, 0.7847, 0.7882, 0.7917, 0.7396, 0.7778, 0.7118, 0.7535, 0.7882, 0.7708, 0.7465, 0.6528, 0.7049, 0.6458, 0.8194, 0.7431, 0.6771, 0.7118, 0.7569, 0.7535, 0.7222, 0.6944, 0.8194, 0.7882, 0.7917, 0.8056, 0.8681, 0.7951, 0.7986, 0.7882, 0.7535, 0.684, 0.7604, 0.7222, 0.7465, 0.7604, 0.8299, 0.7639, 0.816, 0.7153, 0.7292, 0.7083, 0.7431, 0.7014, 0.7222, 0.7812, 0.7292, 0.8056, 0.7118, 0.7882, 0.7431, 0.8125, 0.8299, 0.7292, 0.7326, 0.8056, 0.691, 0.7812, 0.8056, 0.7812, 0.7778, 0.7569, 0.7986, 0.8056, 0.7396, 0.7951, 0.7847, 0.7569, 0.7396, 0.6979, 0.7743, 0.7049, 0.6528, 0.8021, 0.7604, 0.7222, 0.7917, 0.7951, 0.8056, 0.7222, 0.75, 0.6736, 0.7292, 0.7917, 0.7847, 0.75, 0.7014, 0.7743, 0.7257, 0.7882, 0.8299, 0.8438, 0.7951, 0.8333, 0.7361, 0.7049, 0.7604, 0.8333, 0.8438, 0.7465, 0.7917, 0.75, 0.8681, 0.7257, 0.7882, 0.8264, 0.7222, 0.7569, 0.7951, 0.7951, 0.7917, 0.7014, 0.7396, 0.7083, 0.7049, 0.75, 0.6354, 0.75, 0.8333, 0.6875, 0.8299, 0.6875, 0.8229], [0.8785, 0.7674, 0.8368, 0.8125, 0.7847, 0.7639, 0.8125, 0.7882, 0.7361, 0.8125, 0.809, 0.7917, 0.809, 0.7361, 0.8194, 0.7361, 0.8194, 0.7986, 0.7396, 0.7812, 0.7847, 0.816, 0.7882, 0.8229, 0.8889, 0.8229, 0.8194, 0.7882, 0.875, 0.8368, 0.8542, 0.8021, 0.816, 0.8299, 0.8125, 0.8889, 0.8194, 0.7812, 0.8368, 0.8021, 0.8299, 0.809, 0.8021, 0.7743, 0.7847, 0.7778, 0.7674, 0.8229, 0.7118, 0.8194, 0.75, 0.8056, 0.8229, 0.8299, 0.8507, 0.7326, 0.7812, 0.8125, 0.7292, 0.7847, 0.7951, 0.7951, 0.7986, 0.8438, 0.7465, 0.8507, 0.8576, 0.8229, 0.8021, 0.816, 0.7465, 0.7188, 0.8438, 0.7604, 0.7847, 0.816, 0.816, 0.7917, 0.7708, 0.8229, 0.8194, 0.7917, 0.7812, 0.7535, 0.875, 0.8194, 0.7847, 0.7639, 0.7222, 0.8611, 0.809, 0.8194, 0.8438, 0.8576, 0.8438, 0.7812, 0.8264, 0.7569, 0.8542, 0.8889, 0.8403, 0.7674, 0.8472, 0.7535, 0.8715, 0.7604, 0.8611, 0.8542, 0.7639, 0.7882, 0.809, 0.8264, 0.7812, 0.7674, 0.7743, 0.7292, 0.7812, 0.7917, 0.7778, 0.7882, 0.8021, 0.8333, 0.816, 0.7847, 0.8299], [0.8854, 0.8264, 0.8924, 0.875, 0.8125, 0.7812, 0.8333, 0.8333, 0.8472, 0.8681, 0.8681, 0.8611, 0.8646, 0.8021, 0.8368, 0.8333, 0.8472, 0.8299, 0.7604, 0.8715, 0.8194, 0.8333, 0.8438, 0.8681, 0.8785, 0.8715, 0.8958, 0.8438, 0.8785, 0.816, 0.8854, 0.8472, 0.8056, 0.8715, 0.8576, 0.9167, 0.8333, 0.8958, 0.8472, 0.8403, 0.8681, 0.816, 0.8368, 0.8264, 0.816, 0.8681, 0.8472, 0.8403, 0.8194, 0.7951, 0.8368, 0.8264, 0.816, 0.8542, 0.9097, 0.816, 0.7882, 0.8646, 0.809, 0.8576, 0.8264, 0.8229, 0.8715, 0.9062, 0.8229, 0.875, 0.8854, 0.8472, 0.8333, 0.8785, 0.8264, 0.7882, 0.7847, 0.8993, 0.8264, 0.8472, 0.8264, 0.8229, 0.8472, 0.8438, 0.8785, 0.8854, 0.8472, 0.8472, 0.8785, 0.8194, 0.8194, 0.809, 0.7986, 0.8993, 0.8681, 0.8507, 0.8958, 0.8542, 0.8785, 0.809, 0.8715, 0.8264, 0.8438, 0.875, 0.8715, 0.8333, 0.875, 0.8299, 0.8715, 0.8021, 0.9132, 0.8299, 0.8438, 0.8924, 0.8646, 0.8438, 0.8715, 0.8264, 0.8438, 0.8542, 0.8438, 0.7917, 0.7951, 0.8021, 0.875, 0.8333, 0.8576, 0.809, 0.7986], [0.8924, 0.816, 0.9132, 0.8715, 0.8542, 0.8299, 0.8854, 0.8299, 0.9028, 0.934, 0.8854, 0.8854, 0.875, 0.8194, 0.8194, 0.8958, 0.8611, 0.8576, 0.809, 0.9028, 0.8854, 0.8889, 0.8472, 0.8854, 0.8472, 0.8854, 0.9097, 0.8889, 0.8715, 0.8333, 0.8924, 0.8958, 0.8646, 0.9132, 0.8507, 0.9132, 0.8507, 0.9097, 0.8576, 0.8576, 0.8681, 0.8438, 0.8438, 0.8681, 0.8715, 0.9062, 0.8611, 0.8542, 0.8264, 0.8611, 0.8819, 0.8854, 0.8715, 0.875, 0.9132, 0.8403, 0.8299, 0.8889, 0.8542, 0.8785, 0.8681, 0.8681, 0.9167, 0.8889, 0.8264, 0.8542, 0.9097, 0.8542, 0.8819, 0.8681, 0.8819, 0.8854, 0.7986, 0.9271, 0.8958, 0.8681, 0.8576, 0.9028, 0.8993, 0.9097, 0.9306, 0.9028, 0.8924, 0.8611, 0.8958, 0.8368, 0.8472, 0.8194, 0.8472, 0.9132, 0.8819, 0.9062, 0.8889, 0.8715, 0.9271, 0.8368, 0.9028, 0.809, 0.8889, 0.9306, 0.8958, 0.8472, 0.8576, 0.8681, 0.8993, 0.8542, 0.9132, 0.9167, 0.8993, 0.9167, 0.8576, 0.8646, 0.8958, 0.8576, 0.8681, 0.8576, 0.8403, 0.8715, 0.8368, 0.8542, 0.9028, 0.8958, 0.8438, 0.8646, 0.8611], [0.9167, 0.8576, 0.9444, 0.9062, 0.8854, 0.8576, 0.9062, 0.8819, 0.9167, 0.9236, 0.8889, 0.8889, 0.9097, 0.8542, 0.8403, 0.8854, 0.8924, 0.8819, 0.8611, 0.9201, 0.9132, 0.9028, 0.9167, 0.9062, 0.8854, 0.8993, 0.934, 0.8993, 0.8924, 0.875, 0.9132, 0.8924, 0.8924, 0.941, 0.8958, 0.8958, 0.8889, 0.8958, 0.8785, 0.8958, 0.9132, 0.8924, 0.8819, 0.8889, 0.8889, 0.8854, 0.8993, 0.8993, 0.8993, 0.8299, 0.9062, 0.934, 0.875, 0.9271, 0.9201, 0.8958, 0.8576, 0.9097, 0.8611, 0.8993, 0.9028, 0.9132, 0.9479, 0.9167, 0.8819, 0.8785, 0.9201, 0.9097, 0.9028, 0.8854, 0.9028, 0.8958, 0.8264, 0.9514, 0.9062, 0.8854, 0.8819, 0.9201, 0.9444, 0.9444, 0.9236, 0.9132, 0.8924, 0.8681, 0.9201, 0.875, 0.8819, 0.8542, 0.9132, 0.9271, 0.934, 0.8924, 0.9167, 0.8854, 0.9479, 0.8993, 0.9201, 0.8264, 0.8924, 0.9271, 0.9132, 0.8785, 0.9062, 0.8715, 0.8993, 0.8924, 0.9097, 0.9271, 0.9062, 0.9236, 0.9028, 0.8819, 0.9375, 0.8715, 0.8924, 0.9097, 0.8785, 0.9097, 0.8854, 0.8646, 0.9479, 0.9236, 0.9236, 0.9097, 0.8958], [0.941, 0.9167, 0.941, 0.9236, 0.9097, 0.8889, 0.9236, 0.8958, 0.9236, 0.9514, 0.941, 0.9062, 0.9167, 0.9028, 0.9201, 0.9132, 0.9201, 0.9062, 0.9167, 0.9201, 0.9236, 0.9132, 0.941, 0.9375, 0.9097, 0.9097, 0.9306, 0.9236, 0.9306, 0.9097, 0.9236, 0.9028, 0.9132, 0.9479, 0.8924, 0.9097, 0.9236, 0.9236, 0.9132, 0.9028, 0.9028, 0.9375, 0.9271, 0.9028, 0.934, 0.8993, 0.9271, 0.8924, 0.9201, 0.8854, 0.9028, 0.9236, 0.9132, 0.9306, 0.9306, 0.8958, 0.8958, 0.9201, 0.9132, 0.8958, 0.9097, 0.9375, 0.9549, 0.9201, 0.9167, 0.9097, 0.9236, 0.9514, 0.9444, 0.8993, 0.934, 0.8993, 0.8819, 0.9583, 0.9306, 0.941, 0.9236, 0.9271, 0.9583, 0.9271, 0.9444, 0.9271, 0.9167, 0.8785, 0.9375, 0.9271, 0.9167, 0.8785, 0.9236, 0.9375, 0.934, 0.941, 0.9236, 0.8993, 0.9688, 0.9271, 0.9306, 0.8472, 0.8889, 0.9375, 0.9271, 0.934, 0.9306, 0.8958, 0.9271, 0.9271, 0.9271, 0.941, 0.9201, 0.941, 0.9236, 0.8993, 0.941, 0.8924, 0.9097, 0.9167, 0.9062, 0.9375, 0.9062, 0.941, 0.9583, 0.9549, 0.941, 0.9271, 0.9097], [0.9583, 0.9375, 0.934, 0.9375, 0.9375, 0.9167, 0.9028, 0.9028, 0.9479, 0.9479, 0.941, 0.9028, 0.9132, 0.9375, 0.941, 0.941, 0.9375, 0.9167, 0.9306, 0.9444, 0.9271, 0.9132, 0.9479, 0.9306, 0.9236, 0.9062, 0.9306, 0.941, 0.934, 0.9167, 0.941, 0.934, 0.9444, 0.9375, 0.9097, 0.9375, 0.9271, 0.9479, 0.9271, 0.9514, 0.9236, 0.9306, 0.9306, 0.9271, 0.934, 0.9271, 0.9306, 0.9028, 0.9201, 0.9306, 0.941, 0.9306, 0.9271, 0.9271, 0.9479, 0.8924, 0.9444, 0.9271, 0.9236, 0.9375, 0.9271, 0.934, 0.9549, 0.9375, 0.9444, 0.934, 0.9375, 0.9583, 0.9688, 0.9132, 0.9444, 0.9201, 0.9167, 0.9618, 0.9479, 0.941, 0.9444, 0.9375, 0.9514, 0.9444, 0.9514, 0.941, 0.9201, 0.8993, 0.9306, 0.9444, 0.9375, 0.9167, 0.9306, 0.9444, 0.9549, 0.9306, 0.9444, 0.9201, 0.9688, 0.941, 0.9306, 0.8715, 0.9062, 0.9444, 0.9444, 0.9375, 0.9375, 0.9271, 0.9201, 0.9444, 0.9444, 0.9514, 0.9167, 0.934, 0.9444, 0.9375, 0.9375, 0.9097, 0.9271, 0.934, 0.9097, 0.9514, 0.9097, 0.9583, 0.9688, 0.9583, 0.9549, 0.9583, 0.9097], [0.9618, 0.934, 0.9201, 0.9306, 0.9444, 0.9236, 0.9097, 0.9167, 0.9549, 0.9514, 0.941, 0.9236, 0.9167, 0.9444, 0.9549, 0.9479, 0.934, 0.9201, 0.941, 0.9549, 0.9271, 0.934, 0.9549, 0.9375, 0.9306, 0.934, 0.941, 0.9583, 0.9479, 0.9271, 0.9444, 0.9549, 0.9444, 0.9514, 0.9444, 0.941, 0.934, 0.9514, 0.9236, 0.9514, 0.9306, 0.9479, 0.9444, 0.9444, 0.9549, 0.9306, 0.9375, 0.9167, 0.941, 0.9479, 0.934, 0.9479, 0.941, 0.941, 0.941, 0.9062, 0.9479, 0.941, 0.9375, 0.9514, 0.941, 0.9479, 0.9653, 0.941, 0.9444, 0.941, 0.9306, 0.9653, 0.9688, 0.9306, 0.9583, 0.9271, 0.941, 0.9549, 0.9444, 0.9479, 0.9583, 0.9688, 0.9479, 0.9444, 0.9583, 0.934, 0.9375, 0.9306, 0.9375, 0.9514, 0.9444, 0.9375, 0.9375, 0.9444, 0.9479, 0.9444, 0.9375, 0.941, 0.9618, 0.9514, 0.941, 0.8958, 0.9271, 0.9549, 0.9549, 0.9479, 0.9514, 0.9167, 0.9236, 0.9688, 0.9479, 0.9618, 0.9306, 0.934, 0.9514, 0.9514, 0.9375, 0.9201, 0.9583, 0.9444, 0.9271, 0.9618, 0.941, 0.9583, 0.9549, 0.9583, 0.9549, 0.9514, 0.9375], [0.9618, 0.941, 0.9479, 0.9514, 0.941, 0.934, 0.9444, 0.934, 0.9618, 0.9583, 0.9479, 0.9306, 0.9271, 0.934, 0.9444, 0.9549, 0.9306, 0.9306, 0.9375, 0.9618, 0.941, 0.9236, 0.9618, 0.9375, 0.9271, 0.9375, 0.9549, 0.9549, 0.9549, 0.9306, 0.9549, 0.9618, 0.9549, 0.9549, 0.9306, 0.9479, 0.9479, 0.9688, 0.9306, 0.9514, 0.941, 0.9653, 0.941, 0.9514, 0.9653, 0.9375, 0.9444, 0.9444, 0.9479, 0.9549, 0.9306, 0.9583, 0.9479, 0.941, 0.9444, 0.9062, 0.9549, 0.9479, 0.9444, 0.9583, 0.9514, 0.9583, 0.9653, 0.9583, 0.941, 0.9444, 0.934, 0.9618, 0.9757, 0.9375, 0.9688, 0.9306, 0.941, 0.9618, 0.9479, 0.9722, 0.9479, 0.9722, 0.9757, 0.9514, 0.9549, 0.9444, 0.9514, 0.9375, 0.9479, 0.9618, 0.9583, 0.9514, 0.9514, 0.9479, 0.9549, 0.9444, 0.941, 0.9479, 0.9583, 0.9514, 0.9479, 0.8993, 0.941, 0.9549, 0.9618, 0.9757, 0.9549, 0.9236, 0.9479, 0.9688, 0.9549, 0.9618, 0.9444, 0.9549, 0.9514, 0.9583, 0.9549, 0.9479, 0.9444, 0.9549, 0.9306, 0.9792, 0.9444, 0.9688, 0.9653, 0.9757, 0.9618, 0.9549, 0.9514], [0.9583, 0.9444, 0.9653, 0.9375, 0.9444, 0.9306, 0.9375, 0.934, 0.9618, 0.9549, 0.9514, 0.941, 0.934, 0.941, 0.9549, 0.9618, 0.9514, 0.9375, 0.941, 0.9653, 0.9306, 0.9306, 0.9583, 0.9479, 0.9375, 0.9271, 0.9618, 0.9549, 0.9618, 0.934, 0.9618, 0.9653, 0.9653, 0.9618, 0.9444, 0.941, 0.9653, 0.9792, 0.9306, 0.9514, 0.9444, 0.9618, 0.9479, 0.9549, 0.9618, 0.941, 0.9583, 0.9479, 0.9479, 0.9583, 0.934, 0.9618, 0.9583, 0.9444, 0.9549, 0.9167, 0.9549, 0.9444, 0.9479, 0.9618, 0.9549, 0.9618, 0.9653, 0.9583, 0.9375, 0.9618, 0.9514, 0.9688, 0.9757, 0.9479, 0.9722, 0.9375, 0.9444, 0.9653, 0.9514, 0.9792, 0.9618, 0.9688, 0.9688, 0.9549, 0.9653, 0.9583, 0.9444, 0.941, 0.9514, 0.9688, 0.9514, 0.9514, 0.9549, 0.9618, 0.9618, 0.9479, 0.941, 0.9514, 0.9688, 0.9618, 0.9549, 0.9062, 0.941, 0.9514, 0.9653, 0.9757, 0.9549, 0.9444, 0.9479, 0.9653, 0.9514, 0.9722, 0.9479, 0.9549, 0.9549, 0.9618, 0.9583, 0.9583, 0.9479, 0.9653, 0.934, 0.9757, 0.9514, 0.9757, 0.9688, 0.9722, 0.9618, 0.941, 0.9514], [0.9618, 0.9514, 0.9514, 0.9375, 0.9479, 0.9306, 0.9444, 0.941, 0.9653, 0.9583, 0.9514, 0.941, 0.9375, 0.9549, 0.9583, 0.9618, 0.9618, 0.941, 0.941, 0.9618, 0.9306, 0.9306, 0.9688, 0.9444, 0.9444, 0.934, 0.9653, 0.9479, 0.9583, 0.941, 0.9618, 0.9583, 0.9688, 0.9757, 0.941, 0.9479, 0.9583, 0.9792, 0.9271, 0.9514, 0.9514, 0.9757, 0.9514, 0.9514, 0.9618, 0.9549, 0.9549, 0.9514, 0.9514, 0.9549, 0.934, 0.9583, 0.9444, 0.9549, 0.9583, 0.9167, 0.9549, 0.941, 0.9479, 0.9618, 0.9514, 0.9618, 0.9757, 0.9653, 0.9479, 0.9618, 0.9479, 0.9653, 0.9826, 0.9375, 0.9653, 0.9549, 0.9479, 0.9653, 0.9583, 0.9757, 0.9688, 0.9722, 0.9722, 0.9583, 0.9688, 0.9479, 0.9479, 0.934, 0.9549, 0.9688, 0.9549, 0.9618, 0.9583, 0.9722, 0.9583, 0.9653, 0.941, 0.9618, 0.9722, 0.9549, 0.9375, 0.9167, 0.9444, 0.9583, 0.9722, 0.9653, 0.9583, 0.9444, 0.9444, 0.9653, 0.9618, 0.9688, 0.9514, 0.9618, 0.9618, 0.9653, 0.9583, 0.9514, 0.9549, 0.9653, 0.9479, 0.9792, 0.9479, 0.9826, 0.9653, 0.9826, 0.9618, 0.9549, 0.9514], [0.9653, 0.9583, 0.9514, 0.9444, 0.9514, 0.9375, 0.9444, 0.9444, 0.9583, 0.9653, 0.9549, 0.941, 0.9514, 0.9618, 0.9618, 0.9618, 0.9653, 0.9479, 0.9479, 0.9549, 0.9444, 0.9271, 0.9653, 0.9444, 0.9479, 0.9375, 0.9653, 0.9549, 0.9583, 0.9444, 0.9618, 0.9618, 0.9688, 0.9826, 0.941, 0.9479, 0.9583, 0.9688, 0.9271, 0.9549, 0.9549, 0.9826, 0.9549, 0.9549, 0.9618, 0.9618, 0.9618, 0.9549, 0.9653, 0.9549, 0.941, 0.9583, 0.9479, 0.9549, 0.9549, 0.9271, 0.9549, 0.9444, 0.9583, 0.9653, 0.9583, 0.9826, 0.9757, 0.9618, 0.9618, 0.9618, 0.9479, 0.9722, 0.9826, 0.9549, 0.9722, 0.9514, 0.9514, 0.9688, 0.9583, 0.9826, 0.9583, 0.9792, 0.9688, 0.9583, 0.9653, 0.9479, 0.9514, 0.9479, 0.9444, 0.9653, 0.9549, 0.9583, 0.9549, 0.9722, 0.9618, 0.9653, 0.9444, 0.9479, 0.9722, 0.9583, 0.9375, 0.9132, 0.941, 0.9549, 0.9757, 0.9722, 0.9618, 0.941, 0.9479, 0.9618, 0.9653, 0.9688, 0.9583, 0.9653, 0.9653, 0.9618, 0.9618, 0.9479, 0.9549, 0.9722, 0.9479, 0.9792, 0.9549, 0.9792, 0.9583, 0.9826, 0.9618, 0.9618, 0.9653], [0.9653, 0.9583, 0.9583, 0.9583, 0.9549, 0.9375, 0.9479, 0.9479, 0.9618, 0.9653, 0.9549, 0.9444, 0.9583, 0.9583, 0.9653, 0.9688, 0.9618, 0.9514, 0.9583, 0.9583, 0.9549, 0.9236, 0.9826, 0.9514, 0.9375, 0.9375, 0.9653, 0.9618, 0.9653, 0.9444, 0.9618, 0.9653, 0.9688, 0.9826, 0.9549, 0.941, 0.9618, 0.9722, 0.9375, 0.9618, 0.9514, 0.9861, 0.9549, 0.9549, 0.9583, 0.9688, 0.9618, 0.9549, 0.9618, 0.9583, 0.9514, 0.9653, 0.9479, 0.9549, 0.9583, 0.9306, 0.9583, 0.9444, 0.9549, 0.9653, 0.9583, 0.9896, 0.9792, 0.9722, 0.9549, 0.9653, 0.9549, 0.9722, 0.9792, 0.9583, 0.9792, 0.9514, 0.9549, 0.9688, 0.9583, 0.9826, 0.9583, 0.9792, 0.9653, 0.9618, 0.9653, 0.9514, 0.9549, 0.9549, 0.9444, 0.9688, 0.9653, 0.9583, 0.9618, 0.9722, 0.9757, 0.9688, 0.9444, 0.9549, 0.9792, 0.9549, 0.934, 0.9271, 0.9514, 0.9583, 0.9757, 0.9688, 0.9653, 0.9444, 0.9653, 0.9722, 0.9653, 0.9722, 0.9583, 0.9722, 0.9618, 0.9653, 0.9618, 0.9549, 0.9583, 0.9722, 0.9514, 0.9931, 0.9618, 0.9792, 0.9653, 0.9826, 0.9653, 0.9618, 0.9653], [0.9688, 0.9618, 0.9583, 0.9514, 0.9583, 0.9514, 0.9618, 0.9479, 0.9618, 0.9618, 0.9618, 0.9583, 0.9549, 0.9618, 0.9653, 0.9688, 0.9757, 0.9618, 0.9583, 0.9653, 0.9514, 0.9479, 0.9826, 0.9653, 0.9549, 0.934, 0.9688, 0.9618, 0.9653, 0.9479, 0.9653, 0.9722, 0.9688, 0.9826, 0.9549, 0.9444, 0.9722, 0.9688, 0.9375, 0.9618, 0.9514, 0.9757, 0.9549, 0.9618, 0.9618, 0.9688, 0.9618, 0.9583, 0.9583, 0.9653, 0.9514, 0.9618, 0.9583, 0.9583, 0.9618, 0.934, 0.9549, 0.9444, 0.9583, 0.9653, 0.9514, 0.9861, 0.9792, 0.9722, 0.9583, 0.9722, 0.9549, 0.9722, 0.9896, 0.9583, 0.9826, 0.9549, 0.9583, 0.9722, 0.9688, 0.9861, 0.9549, 0.9792, 0.9653, 0.9618, 0.9653, 0.9514, 0.9549, 0.9514, 0.9514, 0.9688, 0.9618, 0.9583, 0.9583, 0.9792, 0.9722, 0.9653, 0.9444, 0.9583, 0.9792, 0.9549, 0.941, 0.934, 0.9444, 0.9618, 0.9722, 0.9826, 0.9688, 0.9444, 0.9688, 0.9757, 0.9653, 0.9757, 0.9618, 0.9757, 0.9618, 0.9653, 0.9688, 0.9514, 0.9653, 0.9792, 0.9549, 0.9931, 0.9618, 0.9792, 0.9722, 0.9826, 0.9688, 0.9653, 0.9653]]\n",
      "[[0.6094, 0.7719, 0.5875, 0.5781, 0.6562, 0.55, 0.6219, 0.6281, 0.5375, 0.7062, 0.6562, 0.7594, 0.7406, 0.5344, 0.6281, 0.5125, 0.6688, 0.6812, 0.5438, 0.6094, 0.6406, 0.75, 0.7156, 0.6062, 0.6875, 0.7375, 0.7188, 0.6938, 0.5938, 0.6719, 0.5562, 0.6906, 0.6688, 0.6375, 0.7938, 0.6625, 0.7812, 0.7469, 0.7375, 0.6812, 0.6312, 0.7031, 0.6594, 0.5781, 0.7375, 0.6312, 0.7562, 0.6938, 0.6125, 0.725, 0.7312, 0.7031, 0.65, 0.6312, 0.7094, 0.5281, 0.6094, 0.7844, 0.6031, 0.7094, 0.7375, 0.5719, 0.5625, 0.6125, 0.7438, 0.5906, 0.7406, 0.7031, 0.5719, 0.6688, 0.6312, 0.6656, 0.6531, 0.5875, 0.6844, 0.75, 0.7469, 0.5812, 0.6656, 0.6719, 0.5375, 0.7125, 0.6844, 0.5594, 0.7719, 0.725, 0.6156, 0.7781, 0.7062, 0.7062, 0.6125, 0.6125, 0.6906, 0.5344, 0.7406, 0.6531, 0.65, 0.6875, 0.5969, 0.6719, 0.6719, 0.625, 0.7125, 0.5188, 0.7156, 0.5844, 0.6344, 0.7281, 0.5625, 0.6875, 0.6438, 0.7125, 0.6906, 0.6219, 0.7469, 0.6219, 0.6406, 0.7094, 0.6031, 0.6844, 0.6719, 0.5938, 0.7156, 0.6312, 0.7281], [0.7344, 0.7844, 0.6688, 0.6156, 0.7438, 0.6781, 0.7906, 0.7062, 0.6219, 0.7625, 0.7375, 0.7875, 0.7719, 0.6062, 0.6938, 0.5906, 0.7062, 0.7344, 0.6719, 0.6, 0.6562, 0.7781, 0.7406, 0.6406, 0.6844, 0.8062, 0.7812, 0.7531, 0.7219, 0.7688, 0.6312, 0.7531, 0.7094, 0.6531, 0.7906, 0.6625, 0.8125, 0.7719, 0.7938, 0.7938, 0.7281, 0.725, 0.7094, 0.6719, 0.7312, 0.65, 0.725, 0.7062, 0.6375, 0.7688, 0.7719, 0.6562, 0.6938, 0.7562, 0.7562, 0.6188, 0.7, 0.7875, 0.675, 0.7094, 0.75, 0.6, 0.7156, 0.6844, 0.7875, 0.7438, 0.7125, 0.7188, 0.7812, 0.7188, 0.7031, 0.6719, 0.7031, 0.6375, 0.6406, 0.7156, 0.7031, 0.7094, 0.7812, 0.7719, 0.6844, 0.6594, 0.7156, 0.625, 0.8031, 0.7656, 0.7562, 0.7875, 0.7188, 0.6969, 0.7125, 0.7469, 0.7812, 0.6938, 0.7469, 0.6969, 0.7219, 0.7312, 0.6656, 0.6969, 0.6812, 0.7, 0.8062, 0.6625, 0.7875, 0.675, 0.6469, 0.7531, 0.6156, 0.8125, 0.6938, 0.8344, 0.6969, 0.7094, 0.7969, 0.725, 0.6844, 0.7406, 0.6594, 0.7375, 0.7562, 0.7062, 0.7625, 0.6594, 0.7594], [0.8094, 0.8375, 0.7781, 0.6969, 0.775, 0.8062, 0.8094, 0.7531, 0.6969, 0.8094, 0.7812, 0.7406, 0.7375, 0.6719, 0.6844, 0.6531, 0.7656, 0.7781, 0.7, 0.7125, 0.7375, 0.8125, 0.7438, 0.7062, 0.7469, 0.8031, 0.7875, 0.8062, 0.8781, 0.8219, 0.7625, 0.8312, 0.7281, 0.7375, 0.85, 0.7094, 0.8125, 0.7719, 0.8781, 0.7438, 0.825, 0.7156, 0.7938, 0.7656, 0.7844, 0.6812, 0.75, 0.7219, 0.7469, 0.8094, 0.7219, 0.7625, 0.7281, 0.8344, 0.8438, 0.7531, 0.7594, 0.8125, 0.7438, 0.7812, 0.8375, 0.7438, 0.775, 0.7312, 0.8062, 0.7812, 0.7844, 0.7344, 0.8562, 0.7812, 0.7469, 0.7719, 0.7562, 0.7281, 0.6375, 0.7875, 0.7688, 0.6969, 0.8281, 0.8344, 0.7938, 0.7125, 0.7125, 0.6875, 0.8312, 0.7719, 0.7625, 0.7469, 0.7062, 0.7531, 0.7125, 0.725, 0.8656, 0.8188, 0.8312, 0.8156, 0.7281, 0.7656, 0.7094, 0.8562, 0.8438, 0.7531, 0.8094, 0.7219, 0.8469, 0.7344, 0.7156, 0.7969, 0.7375, 0.8281, 0.8156, 0.7938, 0.7344, 0.6969, 0.7875, 0.7312, 0.7531, 0.7688, 0.6938, 0.7594, 0.7688, 0.6938, 0.8062, 0.7125, 0.8], [0.8594, 0.7531, 0.8438, 0.7469, 0.7781, 0.8, 0.8562, 0.8188, 0.7281, 0.8562, 0.7938, 0.7562, 0.8094, 0.7031, 0.7594, 0.7156, 0.8344, 0.8312, 0.7406, 0.7469, 0.7875, 0.8375, 0.8, 0.8281, 0.85, 0.8375, 0.8125, 0.7875, 0.9188, 0.8375, 0.8031, 0.8406, 0.8031, 0.8406, 0.8688, 0.8844, 0.8281, 0.7844, 0.8969, 0.8156, 0.8531, 0.8, 0.8531, 0.8156, 0.8281, 0.7469, 0.7656, 0.7938, 0.7656, 0.8094, 0.8031, 0.8, 0.8219, 0.85, 0.8438, 0.7562, 0.8, 0.8438, 0.7812, 0.8, 0.8469, 0.7719, 0.7406, 0.775, 0.85, 0.825, 0.8844, 0.8094, 0.8469, 0.85, 0.7562, 0.7562, 0.875, 0.7844, 0.7656, 0.7906, 0.7906, 0.8125, 0.8406, 0.825, 0.8, 0.8031, 0.7562, 0.7719, 0.8938, 0.7938, 0.7719, 0.7781, 0.7062, 0.8625, 0.8031, 0.7875, 0.8656, 0.8562, 0.875, 0.7625, 0.8469, 0.8, 0.8344, 0.8906, 0.825, 0.8156, 0.8375, 0.7719, 0.8406, 0.7688, 0.8375, 0.8188, 0.7656, 0.8438, 0.8219, 0.8188, 0.7625, 0.7844, 0.8125, 0.7594, 0.8188, 0.8, 0.8, 0.7812, 0.7656, 0.8156, 0.7844, 0.7625, 0.8188], [0.8656, 0.8281, 0.8781, 0.8312, 0.825, 0.8062, 0.8906, 0.8281, 0.8094, 0.8656, 0.8625, 0.8688, 0.8344, 0.7688, 0.8188, 0.8406, 0.8812, 0.8406, 0.7844, 0.8531, 0.85, 0.8719, 0.8406, 0.8469, 0.8375, 0.9125, 0.875, 0.8406, 0.9094, 0.8531, 0.8562, 0.8469, 0.8375, 0.875, 0.9094, 0.9094, 0.8719, 0.8938, 0.8875, 0.8469, 0.8875, 0.7812, 0.8781, 0.8656, 0.8469, 0.8875, 0.85, 0.7969, 0.8625, 0.7938, 0.8531, 0.8469, 0.8, 0.8875, 0.9156, 0.8688, 0.7938, 0.8719, 0.8469, 0.8594, 0.8531, 0.8375, 0.8312, 0.9062, 0.8625, 0.8781, 0.8906, 0.875, 0.8688, 0.8594, 0.8219, 0.8281, 0.8219, 0.9094, 0.8312, 0.8656, 0.8094, 0.8, 0.8938, 0.8406, 0.8812, 0.8625, 0.8281, 0.8562, 0.8875, 0.7938, 0.8438, 0.8594, 0.7938, 0.8969, 0.85, 0.8312, 0.8781, 0.85, 0.9, 0.7969, 0.875, 0.8688, 0.8406, 0.8719, 0.8656, 0.8562, 0.8875, 0.8281, 0.8656, 0.8188, 0.875, 0.7875, 0.8188, 0.8812, 0.875, 0.8156, 0.8375, 0.8719, 0.8812, 0.8594, 0.8719, 0.8188, 0.8188, 0.8156, 0.8531, 0.8469, 0.8281, 0.8281, 0.8125], [0.8688, 0.825, 0.8969, 0.8969, 0.8562, 0.8594, 0.9406, 0.825, 0.9, 0.9094, 0.8938, 0.9, 0.8719, 0.8406, 0.7781, 0.8906, 0.9062, 0.8969, 0.825, 0.8812, 0.9094, 0.9, 0.8688, 0.9125, 0.8625, 0.9375, 0.8656, 0.85, 0.8969, 0.8812, 0.8625, 0.9156, 0.8781, 0.925, 0.8906, 0.925, 0.8812, 0.9156, 0.8906, 0.8656, 0.9188, 0.8406, 0.8719, 0.8812, 0.8844, 0.9125, 0.8469, 0.8344, 0.8844, 0.8844, 0.8719, 0.8969, 0.8625, 0.8906, 0.925, 0.8938, 0.8469, 0.9, 0.8812, 0.8969, 0.9, 0.8875, 0.8875, 0.8906, 0.8688, 0.8656, 0.9375, 0.8656, 0.9219, 0.8656, 0.8719, 0.9031, 0.8219, 0.9438, 0.8844, 0.8938, 0.8594, 0.8938, 0.9219, 0.8844, 0.9094, 0.8719, 0.8875, 0.8938, 0.8781, 0.8281, 0.8875, 0.8656, 0.8469, 0.8906, 0.8906, 0.8719, 0.9062, 0.8875, 0.9312, 0.8469, 0.8719, 0.8719, 0.9, 0.9125, 0.8594, 0.875, 0.8938, 0.8719, 0.8719, 0.8625, 0.8781, 0.8938, 0.9062, 0.9312, 0.8688, 0.8906, 0.8781, 0.8844, 0.8719, 0.875, 0.8938, 0.8562, 0.8438, 0.8531, 0.8812, 0.8906, 0.85, 0.9125, 0.875], [0.8938, 0.875, 0.9219, 0.8812, 0.9312, 0.9031, 0.9469, 0.9, 0.925, 0.9281, 0.9156, 0.9188, 0.9031, 0.8656, 0.8094, 0.9188, 0.925, 0.9125, 0.8844, 0.9062, 0.9375, 0.9281, 0.9188, 0.9406, 0.9219, 0.9219, 0.9031, 0.9188, 0.9, 0.9219, 0.8906, 0.9219, 0.9406, 0.95, 0.9094, 0.9188, 0.9281, 0.9219, 0.9062, 0.8938, 0.9438, 0.8969, 0.9062, 0.9, 0.9062, 0.9156, 0.8875, 0.8875, 0.9156, 0.8719, 0.8969, 0.925, 0.875, 0.9344, 0.9219, 0.9562, 0.875, 0.9094, 0.8969, 0.9094, 0.9094, 0.9156, 0.9125, 0.9188, 0.9188, 0.8562, 0.9406, 0.9188, 0.9375, 0.8875, 0.9312, 0.925, 0.85, 0.95, 0.9188, 0.8938, 0.85, 0.9031, 0.9531, 0.9375, 0.9125, 0.9156, 0.9031, 0.875, 0.9094, 0.8594, 0.9031, 0.8906, 0.9062, 0.9094, 0.9375, 0.8906, 0.9062, 0.9156, 0.9219, 0.9125, 0.9219, 0.8875, 0.9125, 0.9125, 0.8906, 0.8938, 0.9156, 0.8969, 0.8969, 0.9031, 0.875, 0.8875, 0.9219, 0.9375, 0.9062, 0.9062, 0.9219, 0.8969, 0.8781, 0.9062, 0.9062, 0.8844, 0.8719, 0.8562, 0.9375, 0.9156, 0.9, 0.9406, 0.8875], [0.9312, 0.9188, 0.9344, 0.9188, 0.9406, 0.95, 0.9594, 0.9094, 0.9281, 0.9469, 0.95, 0.925, 0.9312, 0.9344, 0.8688, 0.9312, 0.9406, 0.9375, 0.9156, 0.9562, 0.9438, 0.95, 0.9375, 0.9594, 0.9438, 0.9375, 0.9219, 0.9344, 0.9375, 0.9344, 0.925, 0.925, 0.9438, 0.9375, 0.9156, 0.9312, 0.95, 0.9406, 0.925, 0.9062, 0.9406, 0.925, 0.9344, 0.9219, 0.9438, 0.9281, 0.9281, 0.9062, 0.9375, 0.9188, 0.9344, 0.9281, 0.9, 0.9469, 0.9375, 0.95, 0.9188, 0.9281, 0.9375, 0.9344, 0.9562, 0.925, 0.95, 0.9312, 0.9281, 0.8906, 0.95, 0.9375, 0.95, 0.9312, 0.9312, 0.9469, 0.9062, 0.9406, 0.9469, 0.9094, 0.9031, 0.9406, 0.9656, 0.9281, 0.9219, 0.9188, 0.9469, 0.9, 0.925, 0.9375, 0.9375, 0.9281, 0.9562, 0.9281, 0.9469, 0.9062, 0.8906, 0.9375, 0.95, 0.9188, 0.9406, 0.8906, 0.9125, 0.925, 0.9062, 0.9312, 0.9312, 0.9156, 0.9094, 0.9312, 0.9094, 0.9281, 0.9344, 0.9438, 0.9375, 0.9281, 0.9312, 0.9062, 0.9, 0.9125, 0.9219, 0.9156, 0.8844, 0.925, 0.9375, 0.9312, 0.9219, 0.9531, 0.9188], [0.9375, 0.9625, 0.9469, 0.9438, 0.9625, 0.9594, 0.9531, 0.9312, 0.9562, 0.9656, 0.95, 0.9344, 0.9281, 0.9438, 0.9062, 0.975, 0.9438, 0.9656, 0.95, 0.9562, 0.9656, 0.9688, 0.9531, 0.9656, 0.975, 0.9312, 0.9531, 0.9406, 0.9531, 0.9344, 0.9469, 0.9406, 0.9469, 0.9438, 0.9312, 0.9656, 0.9406, 0.9531, 0.9375, 0.9531, 0.9531, 0.925, 0.95, 0.9375, 0.95, 0.9281, 0.9344, 0.9219, 0.9469, 0.9406, 0.9438, 0.9531, 0.9531, 0.9531, 0.9469, 0.9594, 0.95, 0.9469, 0.9438, 0.9625, 0.9656, 0.9438, 0.9562, 0.9562, 0.9531, 0.9438, 0.9562, 0.9469, 0.9656, 0.9344, 0.9406, 0.9438, 0.9312, 0.9406, 0.9562, 0.9344, 0.9281, 0.9469, 0.9469, 0.9219, 0.9344, 0.9312, 0.95, 0.925, 0.9375, 0.9344, 0.9562, 0.9406, 0.95, 0.9375, 0.9625, 0.9281, 0.9312, 0.9375, 0.9562, 0.9438, 0.9438, 0.9188, 0.9281, 0.9344, 0.9375, 0.9438, 0.925, 0.9281, 0.9312, 0.9312, 0.9375, 0.9406, 0.9312, 0.9312, 0.95, 0.9344, 0.95, 0.9188, 0.9312, 0.925, 0.9406, 0.9344, 0.8906, 0.9125, 0.95, 0.9312, 0.9375, 0.9531, 0.9219], [0.9656, 0.9594, 0.95, 0.9625, 0.9719, 0.9656, 0.9719, 0.9531, 0.9688, 0.9656, 0.975, 0.9625, 0.9562, 0.9594, 0.9594, 0.9688, 0.9594, 0.9625, 0.9625, 0.9656, 0.9688, 0.9531, 0.9625, 0.9688, 0.9781, 0.9625, 0.9688, 0.9469, 0.95, 0.9531, 0.9531, 0.9469, 0.95, 0.9469, 0.95, 0.9625, 0.9594, 0.9531, 0.9438, 0.9469, 0.9562, 0.9531, 0.9625, 0.9625, 0.9594, 0.9375, 0.9531, 0.9375, 0.9562, 0.9469, 0.9469, 0.9562, 0.9625, 0.9562, 0.9562, 0.95, 0.9625, 0.9531, 0.95, 0.9625, 0.9531, 0.95, 0.9594, 0.9625, 0.9594, 0.9469, 0.9531, 0.9625, 0.9594, 0.9469, 0.9406, 0.9594, 0.9531, 0.9562, 0.95, 0.9469, 0.9562, 0.9594, 0.9438, 0.9344, 0.9406, 0.9312, 0.9594, 0.9406, 0.9531, 0.9406, 0.9594, 0.9406, 0.9625, 0.9438, 0.9594, 0.9375, 0.9281, 0.9531, 0.95, 0.95, 0.9656, 0.9344, 0.9406, 0.95, 0.9438, 0.9562, 0.9375, 0.9188, 0.9344, 0.9344, 0.9438, 0.9406, 0.9344, 0.9281, 0.9562, 0.9344, 0.9562, 0.9375, 0.9469, 0.9438, 0.9406, 0.9438, 0.9094, 0.9312, 0.9469, 0.9406, 0.9312, 0.9594, 0.9406], [0.9656, 0.9719, 0.975, 0.975, 0.9688, 0.9719, 0.9688, 0.9625, 0.9781, 0.9719, 0.975, 0.975, 0.9719, 0.9594, 0.9688, 0.9781, 0.9594, 0.9688, 0.9781, 0.9688, 0.9781, 0.9688, 0.9688, 0.9719, 0.9719, 0.9625, 0.9625, 0.9562, 0.9531, 0.9594, 0.9594, 0.9469, 0.9531, 0.9469, 0.9562, 0.9688, 0.9531, 0.9594, 0.9594, 0.9469, 0.9688, 0.9656, 0.9594, 0.9531, 0.9688, 0.9469, 0.9531, 0.9625, 0.9531, 0.9562, 0.95, 0.9625, 0.9625, 0.9562, 0.9594, 0.9562, 0.9656, 0.9438, 0.9531, 0.9594, 0.9594, 0.95, 0.9656, 0.9656, 0.9656, 0.9625, 0.9656, 0.9625, 0.9625, 0.9469, 0.9469, 0.9594, 0.9562, 0.9656, 0.9656, 0.9562, 0.9594, 0.9594, 0.9625, 0.9438, 0.9344, 0.9562, 0.9625, 0.9406, 0.9594, 0.9438, 0.9531, 0.9625, 0.9562, 0.9375, 0.9625, 0.9406, 0.95, 0.9531, 0.9562, 0.95, 0.9594, 0.9406, 0.95, 0.9438, 0.9438, 0.9594, 0.9406, 0.9344, 0.9469, 0.9438, 0.9406, 0.9438, 0.9469, 0.9375, 0.95, 0.9594, 0.9562, 0.9469, 0.95, 0.9625, 0.9562, 0.9562, 0.9406, 0.95, 0.9562, 0.9375, 0.95, 0.9469, 0.9531], [0.9719, 0.9688, 0.9781, 0.9781, 0.9688, 0.9719, 0.975, 0.9688, 0.9719, 0.9688, 0.9781, 0.975, 0.9688, 0.9656, 0.975, 0.9781, 0.9719, 0.9719, 0.975, 0.975, 0.9719, 0.9781, 0.9719, 0.9781, 0.9656, 0.9688, 0.9625, 0.9594, 0.9688, 0.9625, 0.9625, 0.9531, 0.9625, 0.9594, 0.9594, 0.9688, 0.9688, 0.9656, 0.9719, 0.9469, 0.9688, 0.9688, 0.9656, 0.9625, 0.9625, 0.9438, 0.9625, 0.9625, 0.9594, 0.9594, 0.95, 0.9656, 0.9625, 0.9594, 0.9656, 0.9625, 0.9812, 0.95, 0.9562, 0.9625, 0.9719, 0.9562, 0.9625, 0.9781, 0.9594, 0.9625, 0.9719, 0.9625, 0.9719, 0.9531, 0.9625, 0.95, 0.9594, 0.9656, 0.9625, 0.9562, 0.9531, 0.9625, 0.9625, 0.9562, 0.9438, 0.9625, 0.9656, 0.9531, 0.9625, 0.9562, 0.9562, 0.9531, 0.9562, 0.95, 0.9625, 0.9531, 0.95, 0.9594, 0.9562, 0.9562, 0.9625, 0.9531, 0.95, 0.9469, 0.95, 0.9562, 0.9438, 0.9375, 0.9469, 0.9438, 0.9469, 0.9531, 0.9469, 0.9375, 0.9531, 0.9625, 0.9469, 0.95, 0.9562, 0.95, 0.9469, 0.9562, 0.95, 0.9562, 0.9531, 0.9406, 0.9438, 0.9438, 0.9562], [0.9781, 0.9688, 0.9781, 0.975, 0.9656, 0.975, 0.975, 0.9688, 0.9719, 0.9688, 0.9812, 0.9781, 0.9625, 0.975, 0.9719, 0.9812, 0.9688, 0.975, 0.9781, 0.9656, 0.975, 0.9781, 0.9688, 0.9781, 0.9781, 0.9688, 0.9688, 0.9625, 0.9625, 0.9625, 0.9625, 0.9656, 0.9656, 0.9625, 0.9625, 0.9656, 0.9656, 0.9562, 0.9625, 0.9625, 0.9688, 0.9719, 0.9688, 0.9688, 0.9562, 0.9562, 0.9594, 0.9719, 0.9656, 0.9688, 0.9562, 0.9656, 0.9656, 0.9719, 0.9719, 0.9625, 0.9719, 0.9625, 0.9656, 0.9625, 0.9688, 0.9656, 0.9688, 0.975, 0.9625, 0.9562, 0.9688, 0.9656, 0.9781, 0.9594, 0.9688, 0.9625, 0.9625, 0.9688, 0.9656, 0.9625, 0.9594, 0.9656, 0.9625, 0.9625, 0.9562, 0.9531, 0.9594, 0.95, 0.9562, 0.9562, 0.95, 0.9656, 0.9562, 0.9531, 0.9594, 0.9625, 0.9531, 0.9594, 0.9562, 0.9531, 0.9562, 0.9562, 0.9594, 0.9562, 0.9656, 0.9531, 0.9531, 0.9438, 0.9531, 0.9406, 0.95, 0.95, 0.9438, 0.9375, 0.9594, 0.9594, 0.9469, 0.9625, 0.9594, 0.9531, 0.9562, 0.9562, 0.9531, 0.95, 0.9469, 0.95, 0.9438, 0.9625, 0.9594], [0.9719, 0.9719, 0.9781, 0.9781, 0.9781, 0.975, 0.9812, 0.975, 0.975, 0.975, 0.9781, 0.9781, 0.9656, 0.9812, 0.9719, 0.9781, 0.9688, 0.975, 0.975, 0.9781, 0.9781, 0.9781, 0.9688, 0.9812, 0.9719, 0.9656, 0.9688, 0.9688, 0.9688, 0.9656, 0.9688, 0.9688, 0.9656, 0.9719, 0.9625, 0.9656, 0.9656, 0.975, 0.9688, 0.9656, 0.9688, 0.9719, 0.9719, 0.9656, 0.9719, 0.9688, 0.9625, 0.9719, 0.9719, 0.9688, 0.9625, 0.9688, 0.9719, 0.975, 0.9719, 0.9625, 0.9781, 0.9719, 0.9719, 0.9594, 0.9719, 0.9688, 0.9719, 0.975, 0.9625, 0.9594, 0.9688, 0.9656, 0.9781, 0.9594, 0.9688, 0.9594, 0.9656, 0.9656, 0.9656, 0.9594, 0.9594, 0.9625, 0.9562, 0.9656, 0.9594, 0.9594, 0.9656, 0.9594, 0.95, 0.9594, 0.9594, 0.9656, 0.9625, 0.9594, 0.9656, 0.9625, 0.9562, 0.9531, 0.9625, 0.9594, 0.9594, 0.9562, 0.9562, 0.9594, 0.9656, 0.9562, 0.9469, 0.9531, 0.95, 0.9469, 0.9625, 0.9406, 0.9531, 0.9438, 0.9625, 0.9562, 0.9562, 0.9625, 0.9562, 0.9562, 0.9594, 0.95, 0.9531, 0.9531, 0.9469, 0.95, 0.9438, 0.9594, 0.9594], [0.975, 0.9719, 0.9781, 0.9812, 0.975, 0.9781, 0.9781, 0.9719, 0.9781, 0.9781, 0.9781, 0.9781, 0.9719, 0.9781, 0.975, 0.9781, 0.975, 0.975, 0.9781, 0.975, 0.9719, 0.975, 0.9719, 0.975, 0.975, 0.9656, 0.9719, 0.9719, 0.9719, 0.9719, 0.9688, 0.9719, 0.9688, 0.9719, 0.9688, 0.9656, 0.9781, 0.9719, 0.9719, 0.9688, 0.975, 0.975, 0.975, 0.9656, 0.9625, 0.9688, 0.9688, 0.9719, 0.9688, 0.9688, 0.9688, 0.9688, 0.9688, 0.975, 0.9719, 0.9688, 0.9719, 0.975, 0.9688, 0.975, 0.975, 0.975, 0.9781, 0.975, 0.9719, 0.9719, 0.9688, 0.9719, 0.9812, 0.9688, 0.975, 0.9688, 0.9688, 0.975, 0.9719, 0.9562, 0.9656, 0.9625, 0.9594, 0.9656, 0.9625, 0.9656, 0.9625, 0.9562, 0.9594, 0.9594, 0.9594, 0.9656, 0.9594, 0.9625, 0.9625, 0.9656, 0.9531, 0.9594, 0.9625, 0.9594, 0.9562, 0.9562, 0.9562, 0.9625, 0.9656, 0.9594, 0.9562, 0.9594, 0.9531, 0.9531, 0.9562, 0.9562, 0.9531, 0.95, 0.9562, 0.9531, 0.9625, 0.9531, 0.9625, 0.9625, 0.9594, 0.9594, 0.9531, 0.9656, 0.9562, 0.9562, 0.95, 0.9594, 0.9531], [0.9781, 0.975, 0.9812, 0.9781, 0.9781, 0.9781, 0.9812, 0.9781, 0.9781, 0.9781, 0.9781, 0.9781, 0.9781, 0.9781, 0.975, 0.9781, 0.9781, 0.9781, 0.9781, 0.9781, 0.9781, 0.975, 0.975, 0.9781, 0.9781, 0.9719, 0.975, 0.9688, 0.9719, 0.9719, 0.9719, 0.975, 0.9719, 0.9719, 0.9719, 0.9688, 0.975, 0.975, 0.975, 0.9688, 0.9688, 0.975, 0.9719, 0.9688, 0.9656, 0.9719, 0.9719, 0.9688, 0.9719, 0.9688, 0.9719, 0.9719, 0.9719, 0.975, 0.975, 0.9719, 0.9781, 0.975, 0.975, 0.9812, 0.9688, 0.975, 0.975, 0.9781, 0.9719, 0.975, 0.9719, 0.975, 0.9781, 0.9781, 0.975, 0.9781, 0.9719, 0.975, 0.9781, 0.9594, 0.9656, 0.9562, 0.9625, 0.9625, 0.9656, 0.9562, 0.9594, 0.9562, 0.9594, 0.9625, 0.9594, 0.9656, 0.9594, 0.9656, 0.9594, 0.9594, 0.9594, 0.9625, 0.9594, 0.9625, 0.9562, 0.9562, 0.9594, 0.9656, 0.9594, 0.9625, 0.9594, 0.9562, 0.9562, 0.9625, 0.9625, 0.9562, 0.9562, 0.9562, 0.9562, 0.9562, 0.9656, 0.9562, 0.9531, 0.9562, 0.9531, 0.9656, 0.95, 0.9656, 0.9594, 0.9562, 0.9531, 0.9594, 0.9625]]\n"
     ]
    }
   ],
   "source": [
    "anchors, scores_train, scores_valid, scores_test = curve\n",
    "print(anchors)\n",
    "print(scores_train)\n",
    "print(scores_valid)\n",
    "print(scores_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0a919ed-00a9-4e90-b7a4-0f3d623bceb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  16.,   23.,   32.,   45.,   64.,   91.,  128.,  181.,  256.,  362.,\n",
      "         512.,  724., 1024., 1448., 2048., 2588.])\n",
      "tensor([0.9975, 0.9951, 0.9920, 0.9876, 0.9829, 0.9782, 0.9768, 0.9719, 0.9682,\n",
      "        0.9664, 0.9666, 0.9666, 0.9662, 0.9666, 0.9676, 0.9684])\n",
      "tensor([0.6607, 0.7168, 0.7664, 0.8081, 0.8513, 0.8825, 0.9076, 0.9294, 0.9431,\n",
      "        0.9517, 0.9574, 0.9606, 0.9628, 0.9651, 0.9673, 0.9686])\n"
     ]
    }
   ],
   "source": [
    "def dummyLoader(seq_len_maximum = None, device = None, batch_size = None, seq_len = None, num_features = None, hyperparameters = None, single_eval_pos = None, x = None, y = None, z = None):\n",
    "    anchors, scores_train, scores_valid, scores_test = curve\n",
    "    x = th.Tensor(anchors, )\n",
    "    y = th.mean(th.Tensor(scores_train), dim = 1)\n",
    "    y_new = th.mean(th.Tensor(scores_test), dim = 1)\n",
    "    return x, y, y_new\n",
    "\n",
    "x, y ,z = dummyLoader(curve)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f53cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_get_batch_func(prior):\n",
    "    return partial(get_batch_domhan, prior=prior)\n",
    "\n",
    "# function producing batches for PFN training\n",
    "def get_batch_domhan(\n",
    "    batch_size,\n",
    "    seq_len,\n",
    "    num_features,\n",
    "    device=\"cpu\",\n",
    "    noisy_target=True,\n",
    "    **_,\n",
    "):\n",
    "    assert num_features == 1\n",
    "\n",
    "    x = np.arange(1, seq_len + 1)\n",
    "    y_target = np.empty((batch_size, seq_len), dtype=float)\n",
    "    y_noisy = np.empty((batch_size, seq_len), dtype=float)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        curve_func =  [0.9975, 0.9951, 0.9920, 0.9876, 0.9829, 0.9782, 0.9768, 0.9719, 0.9682,\n",
    "        0.9664, 0.9666, 0.9666, 0.9662, 0.9666, 0.9676, 0.9684]# uses numpy rng\n",
    "        if noisy_target:\n",
    "            y_noisy[i] = curve_func[i]\n",
    "            y_target[i] = y_noisy[i]\n",
    "        else:\n",
    "            y_target[i], y_noisy[i] = curve_func[i], curve_func[i]\n",
    "    # turn numpy arrays into correctly shaped torch tensors & move them to device\n",
    "    x = (\n",
    "        th.arange(1, seq_len + 1)\n",
    "        .repeat((num_features, batch_size, 1))\n",
    "        .transpose(2, 0)\n",
    "        .to(device)\n",
    "    )\n",
    "    y_target = th.from_numpy(y_target).transpose(1, 0).to(device)\n",
    "    y_noisy = th.from_numpy(y_noisy).transpose(1, 0).to(device)\n",
    "\n",
    "    # changes\n",
    "    x = x.float()\n",
    "    y_target = y_target.float()\n",
    "    y_noisy = y_noisy.float()\n",
    "\n",
    "    return x, y_noisy, y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c62a497-e65e-4b1f-9712-32ce06a242e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_batch_func = create_get_batch_func(prior = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa7ce7fb-9c9f-4858-ab2e-b86bd48d15c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 y evals to estimate 4 buckets. Cut off the last 0 ys.\n",
      "Using cpu:0 device\n",
      "init dist\n",
      "Not using distributed\n",
      "DataLoader.__dict__ {'num_steps': 100, 'get_batch_kwargs': {'batch_size': 1, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x000001984ECB55E0>, 'seq_len_maximum': 16, 'device': 'cpu:0', 'num_features': 1, 'hyperparameters': {}}, 'num_features': 1}\n",
      "Style definition: None\n",
      "Using a Transformer with 1.72 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mjaic\\miniconda3\\envs\\lcpfn\\lib\\site-packages\\torch\\autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter scale (Tensor of shape ()) of distribution HalfNormal() to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\n0.0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mlcpfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_lcpfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_batch_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_batch_domhan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                         \u001b[49m\u001b[43memsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnum_borders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#THIS HAS SOMETHING TO DO WITH THE NUMBER OF BUCKETS, NEED TO FIGURE OUT TF IS GOING ON WITH\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjaic\\OneDrive\\Desktop\\Delft\\Year 1\\Q3\\Deep Learning\\deep-learning-project\\lcpfn\\train_lcpfn.py:92\u001b[0m, in \u001b[0;36mtrain_lcpfn\u001b[1;34m(get_batch_func, seq_len, emsize, nlayers, num_borders, lr, batch_size, epochs)\u001b[0m\n\u001b[0;32m     54\u001b[0m criterions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     55\u001b[0m     num_features: {\n\u001b[0;32m     56\u001b[0m         num_borders: bar_distribution\u001b[38;5;241m.\u001b[39mFullSupportBarDistribution(bucket_limits)\n\u001b[0;32m     57\u001b[0m     }\n\u001b[0;32m     58\u001b[0m }\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     61\u001b[0m     nlayers\u001b[38;5;241m=\u001b[39mnlayers,\n\u001b[0;32m     62\u001b[0m     priordataloader_class\u001b[38;5;241m=\u001b[39mdataloader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m     train_mixed_precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     90\u001b[0m )\n\u001b[1;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n",
      "File \u001b[1;32mc:\\Users\\mjaic\\OneDrive\\Desktop\\Delft\\Year 1\\Q3\\Deep Learning\\deep-learning-project\\lcpfn\\train.py:305\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(priordataloader_class, criterion, encoder_generator, emsize, nhid, nlayers, nhead, dropout, epochs, steps_per_epoch, batch_size, bptt, lr, weight_decay, warmup_epochs, input_normalization, y_encoder_generator, pos_encoder_generator, decoder, extra_prior_kwargs_dict, scheduler, load_weights_from_this_state_dict, validation_period, single_eval_pos_gen, bptt_extra_samples, gpu_device, aggregate_k_gradients, verbose, style_encoder_generator, epoch_callback, initializer, initialize_with_model, train_mixed_precision, saving_period, checkpoint_file, load_optimizer_from_this_state_dict, output_path, **model_extra_args)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m epochs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    298\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    299\u001b[0m     (\n\u001b[0;32m    300\u001b[0m         total_loss,\n\u001b[0;32m    301\u001b[0m         total_positional_losses,\n\u001b[0;32m    302\u001b[0m         time_to_get_batch,\n\u001b[0;32m    303\u001b[0m         forward_time,\n\u001b[0;32m    304\u001b[0m         step_time,\n\u001b[1;32m--> 305\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m     list_losses\u001b[38;5;241m.\u001b[39mappend(total_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(dl, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m validation_period \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mjaic\\OneDrive\\Desktop\\Delft\\Year 1\\Q3\\Deep Learning\\deep-learning-project\\lcpfn\\train.py:244\u001b[0m, in \u001b[0;36mtrain.<locals>.train_epoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m    239\u001b[0m     losses \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[0;32m    240\u001b[0m         output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_out),\n\u001b[0;32m    241\u001b[0m         targets\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m    242\u001b[0m     )\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 244\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m losses \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39moutput\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m    246\u001b[0m loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m/\u001b[39m aggregate_k_gradients\n",
      "File \u001b[1;32mc:\\Users\\mjaic\\miniconda3\\envs\\lcpfn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mjaic\\OneDrive\\Desktop\\Delft\\Year 1\\Q3\\Deep Learning\\deep-learning-project\\lcpfn\\bar_distribution.py:156\u001b[0m, in \u001b[0;36mFullSupportBarDistribution.forward\u001b[1;34m(self, logits, y)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m#print(bucket_log_probs, logits.shape)\u001b[39;00m\n\u001b[0;32m    154\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m scaled_bucket_log_probs\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,target_sample\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 156\u001b[0m side_normals \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalfnormal_with_p_weight_before\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucket_widths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhalfnormal_with_p_weight_before(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbucket_widths[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# TODO look over it again\u001b[39;00m\n\u001b[0;32m    160\u001b[0m log_probs[target_sample \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m side_normals[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlog_prob((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mborders[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39my[target_sample \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.00000001\u001b[39m)) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbucket_widths[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\mjaic\\OneDrive\\Desktop\\Delft\\Year 1\\Q3\\Deep Learning\\deep-learning-project\\lcpfn\\bar_distribution.py:143\u001b[0m, in \u001b[0;36mFullSupportBarDistribution.halfnormal_with_p_weight_before\u001b[1;34m(range_max, p)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhalfnormal_with_p_weight_before\u001b[39m(range_max,p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.5\u001b[39m):\n\u001b[0;32m    142\u001b[0m     s \u001b[38;5;241m=\u001b[39m range_max \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mHalfNormal(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.\u001b[39m))\u001b[38;5;241m.\u001b[39micdf(torch\u001b[38;5;241m.\u001b[39mtensor(p))\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHalfNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjaic\\miniconda3\\envs\\lcpfn\\lib\\site-packages\\torch\\distributions\\half_normal.py:32\u001b[0m, in \u001b[0;36mHalfNormal.__init__\u001b[1;34m(self, scale, validate_args)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, scale, validate_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     31\u001b[0m     base_dist \u001b[38;5;241m=\u001b[39m Normal(\u001b[38;5;241m0\u001b[39m, scale, validate_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHalfNormal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbase_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAbsTransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjaic\\miniconda3\\envs\\lcpfn\\lib\\site-packages\\torch\\distributions\\transformed_distribution.py:79\u001b[0m, in \u001b[0;36mTransformedDistribution.__init__\u001b[1;34m(self, base_distribution, transforms, validate_args)\u001b[0m\n\u001b[0;32m     77\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m shape[:cut]\n\u001b[0;32m     78\u001b[0m event_shape \u001b[38;5;241m=\u001b[39m shape[cut:]\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTransformedDistribution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjaic\\miniconda3\\envs\\lcpfn\\lib\\site-packages\\torch\\distributions\\distribution.py:55\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     53\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m---> 55\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     56\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m             )\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28msuper\u001b[39m(Distribution, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter scale (Tensor of shape ()) of distribution HalfNormal() to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\n0.0"
     ]
    }
   ],
   "source": [
    "result = lcpfn.train_lcpfn(get_batch_func=get_batch_domhan, \n",
    "                         seq_len=16,\n",
    "                         emsize=256,\n",
    "                         nlayers=3,\n",
    "                         num_borders=4, #THIS HAS SOMETHING TO DO WITH THE NUMBER OF BUCKETS, NEED TO FIGURE OUT TF IS GOING ON WITH\n",
    "                         lr=0.001,\n",
    "                         batch_size=1,\n",
    "                         epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08d3d8-8556-436d-a647-4d837290fb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
